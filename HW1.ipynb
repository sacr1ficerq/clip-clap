{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tNk09d6EqAx3"
   },
   "source": [
    "# ДЗ1. CLAP. Обучение проекции из аудио в текстовое пространство CLIP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Описание задания"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQrsmgbkqXVZ"
   },
   "source": [
    "В этом задании вы построите упрощённый вариант модели CLAP (Contrastive Language-Audio Pretraining):\n",
    "\n",
    "- аудио прогоняется через предобученный аудио-энкодер (например, `LanguageBindAudio`, `CNN14/16` или другой);\n",
    "- текстовое описание пропускается через предобученный текстовый энкодер CLIP;\n",
    "- поверх аудио-векторов обучается линейный адаптер, который отображает аудио в то же пространство, что и текстовые эмбеддинги CLIP;\n",
    "- обучение идёт по *контрастивному лоссу*, все энкодеры заморожены, обучаются только параметры аудио-проекции (и, при желании, температура в лоссе);\n",
    "- качество полученного аудио-текстового пространства оценивается на задаче классификации / retrieval аудио по текстам на `AudioCaps`.\n",
    "\n",
    "Идея оценки: если всё сделано правильно, для аудио и его описания косинусное сходство эмбеддингов будет выше, чем для аудио и нерелевантных текстов.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VJupjjbsrTwJ"
   },
   "source": [
    "**Формулировка задач**\n",
    "\n",
    "0. Выбор аудио-энкодера.\n",
    "   Выберите и обоснуйте предобученный аудио-энбеддер:  \n",
    "   - `LanguageBindAudio`,  \n",
    "   - или CNN-модель (например, PANNs CNN14/16),  \n",
    "   - или другой открытый аудио-энкодер, который выдаёт фиксированный эмбеддинг.\n",
    "\n",
    "1. Подсчёт эмбеддингов.\n",
    "   - Посчитайте аудио-векторы для всех аудио из `AudioCaps` с помощью выбранного энкодера.  \n",
    "   - Посчитайте текстовые векторы для подписей с помощью `CLIP text encoder`.\n",
    "\n",
    "2. Линейная аудио-проекция.\n",
    "   - Реализуйте модель `AudioProjection`, переводящую аудио-эмбеддинг в размерность текстового эмбеддинга CLIP.\n",
    "\n",
    "3. Контрастивное обучение.\n",
    "   - Обучите аудио-проекцию на датасете `AudioCaps` по схеме аудио ↔ текст с контрастивным лоссом.  \n",
    "   - Аудио-энкодер и CLIP должны быть полностью заморожены.\n",
    "\n",
    "4. Оценка качества.\n",
    "   - Оцените качество полученного аудио-текстового пространства на задаче классификации/ретривала аудио:  \n",
    "     для каждого аудио найдите наиболее похожую текстовую подпись в батче/валидации и посчитайте `accuracy@1/3/10`.  \n",
    "   - Сравните результаты с *случайным бейзлайном*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "03jAx63StYdA"
   },
   "source": [
    "### Сеттинг"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nw2WU446ta5Y"
   },
   "source": [
    "> Подготовьте все необходимые импорты и загрузите необходимые данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "SbHqYh_Qt4BL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in DATA_ROOT: ['audiocaps_val_new.tsv', 'audiocaps_val.tsv', 'test_texts.json', 'val_texts.json', 'audiocaps_test_new.tsv', 'audiocaps_train.tsv', 'audiocaps_test.tsv', 'audio', 'audio_embeddings_train.pkl', 'train_processed.pkl']\n"
     ]
    }
   ],
   "source": [
    "# Для загрузки AudioCaps можно воспользоваться этим кодом\n",
    "import os\n",
    "\n",
    "# !gdown --id 1FAVKNWXp5afgoNmclDwnj8j_OFTBRmIb -O audiocaps.zip\n",
    "# !unzip audiocaps -d audiocaps\n",
    "\n",
    "DATA_ROOT = \"data/audiocaps\"\n",
    "print(\"Files in DATA_ROOT:\", os.listdir(DATA_ROOT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QCcgZWsbthgL"
   },
   "source": [
    "### Задание 1. Подготовка аудио- и текстовых энкодеров (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1RbbFApuuXml"
   },
   "source": [
    "В этом задании вам нужно:\n",
    "\n",
    "1. Выбрать аудио-энкодер и инициализировать его.\n",
    "2. Инициализировать текстовый энкодер CLIP. Вы свободны выбирать самостоятельно, какой имеено.\n",
    "3. Заморозить параметры обоих энкодеров (мы не дообучаем их, а учим только линейный адаптер).\n",
    "\n",
    "Вы можете:\n",
    "\n",
    "* использовать `LanguageBindAudio` (потребует установки репозитория и зависимостей);\n",
    "* или подставить свою аудио-модель (главное - чтобы на выходе был вектор фиксированной размерности).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "p7mM2dkqtYDI"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cloexec/Desktop/HSE/multimodal/HW1/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Config:\n",
      "EncoderConfig(audio_encoder_name='laion/clap-htsat-unfused', text_encoder_name='openai/clip-vit-base-patch32', audio_sample_rate=48000, batch_size=32, checkpoint_freq=100)\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from src.model import AudioEncoder, EncoderConfig, TextEncoder\n",
    "\n",
    "config = EncoderConfig(\n",
    "    audio_encoder_name=\"laion/clap-htsat-unfused\",\n",
    "    text_encoder_name=\"openai/clip-vit-base-patch32\",\n",
    ")\n",
    "\n",
    "print(\"\\nConfig:\")\n",
    "print(config)\n",
    "\n",
    "audio_encoder = AudioEncoder(config.audio_encoder_name)\n",
    "text_encoder = TextEncoder(config.text_encoder_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F6BdjCPcvrK7"
   },
   "source": [
    "### Задание 2. Предподсчёт аудио- и текстовых эмбеддингов (3 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5kz9a_VvxCs"
   },
   "source": [
    "> Важный момент, который пригодится вам и в других домашних.\n",
    "\n",
    "Чтобы не тратить время на многократный прогон энкодеров при обучении, следует:\n",
    "\n",
    "1. Предварительно посчитывать аудио-эмбеддинги для каждого `.flac` в train/val/test.\n",
    "2. Записывать их в файл формата `pickle` (например), где ключ - имя файла, значение - numpy-вектор.\n",
    "3. Аналогично посчитать текстовые эмбеддинги для подписей через CLIP и совместить их с аудио.\n",
    "\n",
    "Рекомендуемая структура:\n",
    "\n",
    "* функция `extract_audio_vectors_with_checkpointing(...)` - обходит файлы, считает эмбеддинги, периодически делает чекпоинты;\n",
    "* функция `extract_text_embeddings(texts, clip_model, clip_processor)` - возвращает список текстовых эмбеддингов;\n",
    "* функция `process_dataset(...)` - читает `.tsv`, мержит аудио-эмбеддинги и текстовые, сохраняет список словарей вида  \n",
    "  `{\"uniq_id\": ..., \"audio_embedding\": ..., \"text_embedding\": ...}` в pickle.\n",
    "\n",
    "> Вы вольны отходить от предлагаемой структуры."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "S9OIM6ojkKQk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Audio Embeddings (with checkpointing) from data/audiocaps/audio/train...\n",
      "Loaded 49515 audio embeddings from checkpoint data/audiocaps/audio_embeddings_train.pkl\n",
      "Processing Dataset (merge audio + text)...\n",
      "Processing 49490 samples from audiocaps_train.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|███████████████████████████████████████████████████████████████████████| 1547/1547 [00:38<00:00, 40.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 49490 samples to data/audiocaps/train_processed.pkl\n",
      "Processed dataset with 49490 samples\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "from src.dataset import extract_audio_embeddings_with_checkpointing, process_dataset, AudioTextDataset, get_embedding_dimensions\n",
    "\n",
    "DATA_ROOT = \"data/audiocaps/\"\n",
    "CHECKPOINT_FREQ = 50\n",
    "BATCH_SIZE = 32\n",
    "CHECKPOINT_FILENAME = \"audio_embeddings_train.pkl\"\n",
    "PROCESSED_DATASET_FILENAME = \"train_processed.pkl\"\n",
    "\n",
    "data_root = Path(DATA_ROOT)\n",
    "assert data_root.exists(), \"No data directory found\"\n",
    "\n",
    "audio_dir = data_root / \"audio\" / \"train\"\n",
    "assert audio_dir.exists()\n",
    "\n",
    "assert audio_dir.exists() and list(audio_dir.glob(\"*.flac\")), \"No audio files found in train directory\"\n",
    "\n",
    "tsv_path = data_root / \"audiocaps_train.tsv\"\n",
    "assert tsv_path.exists(), \"No train.tsv file found\"\n",
    "\n",
    "print(f\"Extracting Audio Embeddings (with checkpointing) from {audio_dir}...\")\n",
    "\n",
    "checkpoint_path = data_root / CHECKPOINT_FILENAME\n",
    "\n",
    "if not checkpoint_path.exists():\n",
    "    audio_embeddings = extract_audio_embeddings_with_checkpointing(\n",
    "        audio_dir=audio_dir,\n",
    "        encoder=audio_encoder,\n",
    "        checkpoint_path=checkpoint_path,\n",
    "        checkpoint_freq=CHECKPOINT_FREQ,\n",
    "        resume=True\n",
    "    )\n",
    "    print(f\"Extracted {len(audio_embeddings)} audio embeddings\")\n",
    "else:\n",
    "    audio_embeddings = pickle.load(open(checkpoint_path, 'rb'))\n",
    "    print(f\"Loaded {len(audio_embeddings)} audio embeddings from checkpoint {checkpoint_path}\")\n",
    "\n",
    "print(\"Processing Dataset (merge audio + text)...\")\n",
    "\n",
    "dataset = process_dataset(\n",
    "    tsv_path=tsv_path,\n",
    "    audio_embeddings=audio_embeddings,\n",
    "    text_encoder=text_encoder,\n",
    "    output_path=data_root / PROCESSED_DATASET_FILENAME,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "print(f\"Processed dataset with {len(dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8OD_skxfxBVY"
   },
   "source": [
    "### Задание 3. Линейный аудио-адаптер и контрастивный лосс (3 балла)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sCDte8VBw9ql"
   },
   "source": [
    "Теперь, когда у нас есть пары *audio_embedding, text_embedding*, реализуем:\n",
    "\n",
    "1. Класс `AudioTextDataset`, который читает pickle с комбинированными эмбеддингами.\n",
    "2. Линейную модель `AudioProjection`, переводящую аудио-эмбеддинг в размерность текстового.\n",
    "3. Контрастивный лосс для аудио↔текст:\n",
    "   - нормализовать эмбеддинги по L2;\n",
    "   - посчитать матрицу сходства;\n",
    "   - задать таргеты как `targets = arange(batch_size)`;\n",
    "   - вычислить `CrossEntropyLoss` как для строк audio→text и для строк text→audio, усреднить.\n",
    "\n",
    "Обучаем **только** `AudioProjection` (и, по желанию, параметр temperature).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ypQ42sQ9zs7Q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PyTorch Dataset...\n",
      "Loaded dataset with 49490 samples\n",
      "Great success\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating PyTorch Dataset...\")\n",
    "pytorch_dataset = AudioTextDataset(data_root / PROCESSED_DATASET_FILENAME)\n",
    "\n",
    "print(\"Great success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample:\n",
      "\taudio embedding shape: torch.Size([512])\n",
      "\ttext embedding shape: torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "audio_emb, text_emb = pytorch_dataset[0]\n",
    "print(\"Sample:\")\n",
    "print(f\"\\taudio embedding shape: {audio_emb.shape}\")\n",
    "print(f\"\\ttext embedding shape: {text_emb.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q-kkEEuzx_Vi"
   },
   "source": [
    "### Задание 4. Оценка качества на задаче классификации аудио (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9EQbXbIAyRM-"
   },
   "source": [
    "\n",
    "Теперь нужно понять, насколько хорошо аудио-векторы после проекции \"попадают\" в пространство текстовых эмбеддингов:\n",
    "\n",
    "1. Посчитайте проекции аудио для всех примеров в валидации.\n",
    "2. Для каждого аудио найдите `top-k` наиболее похожих текстов по косинусному сходству (или скалярному произведению после L2-нормализации).\n",
    "3. Посчитайте `accuracy@1`, `accuracy@3`, `accuracy@10`, т.е. долю случаев, когда \"правильный\" текст попал в топ-k.\n",
    "4. Сравните с неким *случайным бейзлайном*: для каждого аудио выберите `k` случайных текстов и посчитайте такую же метрику.\n",
    "\n",
    "> Важно: в батче класс \"правильного\" текста для i-го аудио - это индекс i (как в контрастивном лоссе)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "RFrO9tRoylVl"
   },
   "outputs": [],
   "source": [
    "raise NotImplementedError\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wEoUVkEQxp05",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Вывод"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A1e1D5QMy3yP"
   },
   "source": [
    "Оформите, пожалуйста, небольшой вывод. Например, можно воспрользоваться следующим планом:\n",
    "\n",
    "   * какую аудио-модель вы выбрали и почему;\n",
    "   * как вели себя потери на обучении;\n",
    "   * какие значения метрик получились и насколько они превосходят случайный baseline;\n",
    "   * любые наблюдения (например, зависимость от числа эпох, размера батча и т.д.);\n",
    "   * милые пожелания ассистенту/лектору, который будет это проверять."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q_e9rsiIzhHE"
   },
   "source": [
    "your text here TODO"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "CLOP",
   "language": "python",
   "name": "clop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
